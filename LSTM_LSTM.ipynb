{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LSTM-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FWuU_lCrplX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "dddcbfba-edd2-4c38-87d4-abe9cc2a3933"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3vqvPFwcBI"
      },
      "source": [
        "!mkdir -p \"Gigaword\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cFq6VUkws19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "957e129d-2383-4233-b7e1-cf8b0d5bc9b9"
      },
      "source": [
        "!tar xvzf \"/content/gdrive/My Drive/585/Project/Gigaword/summary.tar.gz\" -C \"Gigaword/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sumdata/\n",
            "sumdata/DUC2004/\n",
            "sumdata/DUC2004/input.txt\n",
            "sumdata/DUC2004/task1_ref3.txt\n",
            "sumdata/DUC2004/task1_ref0.txt\n",
            "sumdata/DUC2004/task1_ref1.txt\n",
            "sumdata/DUC2004/task1_ref2.txt\n",
            "sumdata/Giga/\n",
            "sumdata/Giga/input.txt\n",
            "sumdata/Giga/task1_ref0.txt\n",
            "sumdata/DUC2003/\n",
            "sumdata/DUC2003/input.txt\n",
            "sumdata/DUC2003/task1_ref3.txt\n",
            "sumdata/DUC2003/task1_ref0.txt\n",
            "sumdata/DUC2003/task1_ref1.txt\n",
            "sumdata/DUC2003/task1_ref2.txt\n",
            "sumdata/train/\n",
            "sumdata/train/train.title.txt.gz\n",
            "sumdata/train/valid.article.filter.txt\n",
            "sumdata/train/valid.title.filter.txt\n",
            "sumdata/train/train.article.txt.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1deX2YL4xUhE"
      },
      "source": [
        "!gunzip -k \"/content/Gigaword/sumdata/train/train.article.txt.gz\"\n",
        "!gunzip -k \"/content/Gigaword/sumdata/train/train.title.txt.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJJvVDWz4tYJ"
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/585/Project/Gigaword/val_art.txt\" \"Gigaword/val_art.txt\"\n",
        "!cp \"/content/gdrive/My Drive/585/Project/Gigaword/val_ref.txt\" \"Gigaword/val_ref.txt\"\n",
        "!cp \"/content/gdrive/My Drive/585/Project/Gigaword/test_art.txt\" \"Gigaword/test_art.txt\"\n",
        "!cp \"/content/gdrive/My Drive/585/Project/Gigaword/test_ref.txt\" \"Gigaword/test_ref.txt\"\n",
        "!cp \"/content/gdrive/My Drive/585/Project/Gigaword/vocab.txt\" \"Gigaword/vocab.txt\"\n",
        "!mkdir -p \"gdrive/My Drive/585/Project/saved_models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlRIlcrTjyEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "39c60366-e89a-4571-f473-16d3ce1ad2b3"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBhyhqrcKEgR"
      },
      "source": [
        "import os\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from rouge import Rouge\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKvNgzOxxRt"
      },
      "source": [
        "#Config\n",
        "class config:\n",
        "  train_art_path = \"Gigaword/sumdata/train/train.article.txt\"\n",
        "  train_ref_path = \"Gigaword/sumdata/train/train.title.txt\"\n",
        "  val_art_path = \"Gigaword/val_art.txt\"\n",
        "  val_ref_path = \"Gigaword/val_ref.txt\"\n",
        "  test_art_path = \"Gigaword/test_art.txt\"\n",
        "  test_ref_path = \"Gigaword/test_ref.txt\"\n",
        "  vocab_path = \"Gigaword/vocab.txt\"\n",
        "  model_save_path = \"gdrive/My Drive/585/Project/saved_models\"\n",
        "  \n",
        "  art_max_len = 75\n",
        "  ref_max_len = 30\n",
        "  batch_size = 64\n",
        "  n_hid = 512\n",
        "  beam_size = 4\n",
        "  n_itr = 30000\n",
        "  lr = 0.001\n",
        " \n",
        "  use_attn = True\n",
        "  sampling_type = \"greedy\"\n",
        "  #beam_size = 4\n",
        "  \n",
        "  rand_unif_init_mag = 0.02\n",
        "  trunc_norm_init_std = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBUNtU0kJXnx"
      },
      "source": [
        "\n",
        "config.bos = \"<s>\"\n",
        "config.eos = \"</s>\"\n",
        "config.pad = \"<pad>\"\n",
        "config.unk = \"<unk>\"\n",
        "config.n_vocab = 30000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g2H-ydaVy66"
      },
      "source": [
        "class Vocab(object):\n",
        "  def __init__(self, config, max_size):\n",
        "    self.config = config\n",
        "    self._word_to_id = {}\n",
        "    self._id_to_word = {}\n",
        "    self._count = 0 # keeps track of total number of words in the Vocab\n",
        "\n",
        "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
        "    for w in [config.unk, config.pad, config.bos, config.eos]:\n",
        "      self._word_to_id[w] = self._count\n",
        "      self._id_to_word[self._count] = w\n",
        "      self._count += 1\n",
        "\n",
        "    # Read the vocab file and add words up to max_size\n",
        "    with open(config.vocab_path, 'r') as vocab_f:\n",
        "      for line in vocab_f:\n",
        "        pieces = line.split()\n",
        "        if len(pieces) != 2:\n",
        "          continue\n",
        "        w = pieces[0]\n",
        "        self._word_to_id[w] = self._count\n",
        "        self._id_to_word[self._count] = w\n",
        "        self._count += 1\n",
        "        if max_size != 0 and self._count >= max_size:\n",
        "          # print (\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
        "          break\n",
        "    \n",
        "  def word2id(self, word):\n",
        "    if word not in self._word_to_id:\n",
        "      return self._word_to_id[self.config.unk]\n",
        "    return self._word_to_id[word]\n",
        "\n",
        "  def id2word(self, word_id):\n",
        "    return self._id_to_word[word_id]\n",
        "    \n",
        "config.vocab_obj = Vocab(config, config.n_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKaf4WSRSK_y"
      },
      "source": [
        "class Gigaword_dataset(Dataset):\n",
        "  def __init__(self, art_path, ref_path, config):\n",
        "    self.config = config\n",
        "    self.art_max_len = config.art_max_len\n",
        "    self.ref_max_len = config.ref_max_len\n",
        "    art_itr = open(art_path, \"r\")\n",
        "    ref_itr = open(ref_path, \"r\")\n",
        "    self.pair_list = []\n",
        "    for art in art_itr:\n",
        "      art = art.strip()\n",
        "      ref = next(ref_itr).strip()\n",
        "      if len(art) < 10 or len(ref) < 5:\n",
        "        continue\n",
        "      self.pair_list.append((art, ref))\n",
        "    art_itr.close()\n",
        "    ref_itr.close()\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.pair_list)\n",
        "  \n",
        "  def __getitem__(self,ind):\n",
        "    art, ref = self.pair_list[ind]\n",
        "    art_ids, seq_len, spl_mask = self.tokenize_art(art)\n",
        "    ref_ids = self.tokenize_ref(ref)\n",
        "    art_ids = T.LongTensor(art_ids)\n",
        "    ref_ids = T.LongTensor(ref_ids)\n",
        "    spl_mask = T.LongTensor(spl_mask)\n",
        "    return art_ids, seq_len, ref_ids, spl_mask\n",
        "  \n",
        "  def tokenize_art(self, art):\n",
        "    tokens = art.split(\" \")\n",
        "    art_ids = []\n",
        "    for token in tokens:\n",
        "      art_ids.append(self.config.vocab_obj.word2id(token))\n",
        "    spl_token_mask = [1]*len(art_ids)\n",
        "    seq_len = len(art_ids)\n",
        "    return art_ids, seq_len, spl_token_mask\n",
        "  \n",
        "  def tokenize_ref(self, ref):\n",
        "    tokens = ref.split(\" \")\n",
        "    tokens = tokens[:(self.ref_max_len-2)] #Considering <s> and </s>\n",
        "    tokens = [self.config.bos] + tokens + [self.config.eos]\n",
        "    ref_ids = []\n",
        "    for token in tokens:\n",
        "      ref_ids.append(self.config.vocab_obj.word2id(token))\n",
        "    return ref_ids\n",
        "\n",
        "def collate_fun(batch):\n",
        "  batch_art_ids = [obj[0] for obj in batch]\n",
        "  batch_seq_len = T.LongTensor([obj[1] for obj in batch])\n",
        "  batch_ref_ids = [obj[2] for obj in batch]\n",
        "  batch_spl_mask = [obj[3] for obj in batch]\n",
        "  batch_art_ids = nn.utils.rnn.pad_sequence(batch_art_ids, batch_first=True, padding_value=config.vocab_obj.word2id(config.pad))\n",
        "  batch_ref_ids = nn.utils.rnn.pad_sequence(batch_ref_ids, batch_first=True, padding_value=config.vocab_obj.word2id(config.pad))\n",
        "  batch_spl_mask = nn.utils.rnn.pad_sequence(batch_spl_mask, batch_first=True, padding_value=0)\n",
        "  return batch_art_ids, batch_seq_len, batch_ref_ids, batch_spl_mask\n",
        "\n",
        "def get_train_valid_dataloaders(config):\n",
        "  train_dataset = Gigaword_dataset(config.train_art_path, config.train_ref_path, config)\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size = config.batch_size, \n",
        "                                sampler = train_sampler, num_workers=2,\n",
        "                                collate_fn = collate_fun, drop_last = False\n",
        "                               )\n",
        "  \n",
        "  val_dataset = Gigaword_dataset(config.val_art_path, config.val_ref_path, config)\n",
        "  val_sampler = SequentialSampler(val_dataset)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size = config.batch_size, \n",
        "                                sampler = val_sampler, num_workers=2,\n",
        "                                collate_fn = collate_fun, drop_last = False\n",
        "                               )\n",
        "  return train_dataloader, val_dataloader\n",
        "\n",
        "def get_test_dataloader(config):\n",
        "  test_dataset = Gigaword_dataset(config.test_art_path, config.test_ref_path, config)\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size = config.batch_size, \n",
        "                                sampler = test_sampler, num_workers=2,\n",
        "                                collate_fn = collate_fun, drop_last = False\n",
        "                               )\n",
        "  return test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_6GpfaL4wc-"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class LSTM_Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(LSTM_Encoder, self).__init__()\n",
        "    self.lstm = nn.LSTM(config.n_hid, config.n_hid, num_layers=1, batch_first=True, bidirectional=True)\n",
        "    self.reduce_h = nn.Linear(config.n_hid * 2, config.n_hid)\n",
        "    self.reduce_c = nn.Linear(config.n_hid * 2, config.n_hid)\n",
        "    self.embeds = nn.Embedding(config.n_vocab, config.n_hid, padding_idx = config.vocab_obj.word2id(config.pad))\n",
        "  \n",
        "  def forward(self, batch_art_ids, batch_seq_len):\n",
        "    embeds = self.embeds(batch_art_ids)\n",
        "    packed = pack_padded_sequence(embeds, batch_seq_len, batch_first=True, enforce_sorted=False)\n",
        "    enc_out, enc_hid = self.lstm(packed)\n",
        "    enc_out,_ = pad_packed_sequence(enc_out, batch_first=True)\n",
        "    enc_out = enc_out.contiguous()                              #bs, n_seq, 2*n_hid\n",
        "    h, c = enc_hid                                              #shape of h: 2, bs, n_hid\n",
        "    h = T.cat(list(h), dim=1)                                   #bs, 2*n_hid\n",
        "    c = T.cat(list(c), dim=1)\n",
        "    h_reduced = F.relu(self.reduce_h(h))                        #bs,n_hid\n",
        "    c_reduced = F.relu(self.reduce_c(c))\n",
        "    return enc_out, (h_reduced.unsqueeze(0), c_reduced.unsqueeze(0))\n",
        "  \n",
        "\n",
        "class Encoder_Attention(nn.Module):\n",
        "  #Perform additive attention: scores = v^T([W_e.enc_h+W_d.dec_h]))\n",
        "  def __init__(self, config):\n",
        "    super(Encoder_Attention, self).__init__()\n",
        "    self.W_e = nn.Linear(2*config.n_hid, config.n_hid)\n",
        "    self.W_d = nn.Linear(config.n_hid, config.n_hid)\n",
        "    self.v = nn.Linear(config.n_hid, 1)\n",
        "  \n",
        "  def forward(self, enc_h, dec_h, batch_spl_mask):\n",
        "    enc_h = self.W_e(enc_h) #bs, enc_seq_len, n_hid\n",
        "    dec_h = self.W_d(dec_h) #bs, dec_seq_len, n_hid\n",
        "    scores = self.v(enc_h.unsqueeze(1) + dec_h.unsqueeze(2)).squeeze(3) #bs, dec_seq_len, enc_seq_len\n",
        "    bs, enc_seq_len = batch_spl_mask.size()\n",
        "    dec_seq_len = dec_h.size(1)\n",
        "    enc_attn_mask = batch_spl_mask.unsqueeze(1).expand(bs,dec_seq_len,enc_seq_len)\n",
        "    scores.masked_fill_(enc_attn_mask == 0, -1e9) #Masks [CLS], [SEP], [PAD] tokens\n",
        "    attn_w = F.softmax(scores, dim=2) #bs, dec_seq_len, enc_seq_len\n",
        "    context_vecs = T.bmm(attn_w, enc_h) #bs,dec_seq_len,n_hid\n",
        "    return context_vecs\n",
        "  \n",
        "\n",
        "  \n",
        "class LSTM_Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(LSTM_Decoder, self).__init__()\n",
        "    self.use_attn = config.use_attn\n",
        "    self.lstm = nn.LSTM(config.n_hid, config.n_hid, batch_first=True)\n",
        "    if self.use_attn:\n",
        "      self.fc = nn.Linear(2*config.n_hid, config.n_vocab)\n",
        "      self.attn_model = Encoder_Attention(config)\n",
        "    else:\n",
        "      self.fc = nn.Linear(config.n_hid, config.n_vocab)\n",
        "    \n",
        "  \n",
        "  def forward(self, dec_embeds, enc_word_reps, batch_spl_mask, dec_hidden):\n",
        "    dec_out, dec_hidden = self.lstm(dec_embeds, dec_hidden) # dec_out:- (bs,ref_n_seq-1,n_hid)\n",
        "    if self.use_attn:\n",
        "      context_vec = self.attn_model(enc_word_reps, dec_out, batch_spl_mask) #bs,ref_n_seq-1,n_hid\n",
        "      logits = self.fc(T.cat([dec_out, context_vec], dim=2))\n",
        "    else:\n",
        "      logits = self.fc(dec_out) #bs,ref_n_seq-1,n_vocab\n",
        "    return logits, dec_hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iw5AG6D8qTF"
      },
      "source": [
        "class Beam(object):\n",
        "  def __init__(self, config, enc_sent_rep):\n",
        "    h, c = enc_sent_rep\n",
        "    self.pad_id = config.vocab_obj.word2id(config.pad)\n",
        "    self.bos_id = config.vocab_obj.word2id(config.bos)\n",
        "    self.eos_id = config.vocab_obj.word2id(config.eos)\n",
        "    self.generated_tokens = T.LongTensor(config.beam_size,1).fill_(self.bos_id).to(0)\n",
        "    self.scores = T.FloatTensor(config.beam_size,1).fill_(-1e9).to(0)\n",
        "    self.scores[0][0] = 0\n",
        "    self.hid_h = h.repeat(1,config.beam_size,1) #1,beam_size,n_hid\n",
        "    self.hid_c = c.repeat(1,config.beam_size,1)\n",
        "    self.n_vocab = config.n_vocab\n",
        "    self.beam_size = config.beam_size\n",
        "    self.done = False\n",
        "    \n",
        "  def get_prev_tokens(self):\n",
        "    return self.generated_tokens[:,-1].unsqueeze(1) #beam,1\n",
        "  \n",
        "  def advance(self, log_probs, hidden_vecs):\n",
        "    h, c = hidden_vecs #beam, n_hid each\n",
        "    scores = log_probs + self.scores #beam, n_vocab\n",
        "    scores = scores.view(-1,1) #beam*n_vocab,1\n",
        "    best_scores, best_score_ids = T.topk(input=scores, k=self.beam_size,dim=0)\n",
        "    self.scores = best_scores #(beam,1) sorted\n",
        "    best_beam_inds = best_score_ids.squeeze(1)//self.n_vocab #(beam,) sorted\n",
        "    best_tokens = best_score_ids%self.n_vocab #(beam,1)\n",
        "    self.hid_h = h[best_beam_inds].unsqueeze(0) #1,beam,n_hid\n",
        "    self.hid_c = c[best_beam_inds].unsqueeze(0) #1,beam,n_hid\n",
        "    self.generated_tokens = self.generated_tokens[best_beam_inds] #beam,t-1\n",
        "    self.generated_tokens = T.cat([self.generated_tokens, best_tokens], dim=1) #beam,t\n",
        "    \n",
        "    #End condition is when top-of-beam is EOS.\n",
        "    if best_tokens[0].item() == self.eos_id:\n",
        "      self.done = True\n",
        "    \n",
        "  def get_best(self):\n",
        "    return self.generated_tokens[0]\n",
        "  \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIatQtGIWFv3"
      },
      "source": [
        "class Lstm_Lstm_Processor(object):\n",
        "  def __init__(self, config, mode = \"train\"):\n",
        "    self.n_vocab = config.n_vocab\n",
        "    self.encoder = LSTM_Encoder(config).to(0)\n",
        "    self.decoder = LSTM_Decoder(config).to(0)\n",
        "    if mode == \"train\":\n",
        "      train_dataloader, val_dataloader = get_train_valid_dataloaders(config)\n",
        "      self.train_dataloader, self.val_dataloader = train_dataloader, val_dataloader\n",
        "      self.enc_optimizer = T.optim.Adam(self.encoder.parameters(), lr = config.lr)\n",
        "      self.dec_optimizer = T.optim.Adam(self.decoder.parameters(), lr = config.lr)\n",
        "      self.encoder.train()\n",
        "      self.decoder.train()\n",
        "    else:\n",
        "      self.test_dataloader = get_test_dataloader(config)\n",
        "      self.encoder.eval()\n",
        "      self.decoder.eval()\n",
        "      \n",
        "    self.vocab_obj = config.vocab_obj\n",
        "    self.config = config\n",
        "    self.pad_id = self.vocab_obj.word2id(config.pad)\n",
        "    self.bos_id = self.vocab_obj.word2id(config.bos)\n",
        "    self.eos_id = self.vocab_obj.word2id(config.eos)\n",
        "    \n",
        "  def train_batch(self, batch):\n",
        "    batch_art_ids, batch_seq_len, batch_ref_ids, batch_spl_mask = batch\n",
        "    batch_art_ids = batch_art_ids.to(0) #bs, art_n_seq\n",
        "    batch_ref_ids = batch_ref_ids.to(0) #bs, ref_n_seq\n",
        "    batch_spl_mask = batch_spl_mask.to(0) #bs, art_n_seq\n",
        "    #------Encoding----------\n",
        "    enc_word_reps, enc_sent_rep = self.encoder(batch_art_ids, batch_seq_len)\n",
        "    #------Decoding----------\n",
        "    dec_inp = batch_ref_ids[:, :-1] #ignoring </s>; bs,ref_n_seq-1\n",
        "    dec_target = batch_ref_ids[:, 1:] #ignoring <s>; bs,ref_n_seq-1\n",
        "    dec_hidden = enc_sent_rep\n",
        "    dec_embeds = self.encoder.embeds(dec_inp) #bs,ref_n_seq-1,n_hid\n",
        "    dec_logits, _ = self.decoder(dec_embeds, enc_word_reps, batch_spl_mask, dec_hidden)\n",
        "    #----- Training-----------\n",
        "    dec_target = dec_target.contiguous().view(-1) #(bs*(ref_n_seq-1),)\n",
        "    dec_logits = dec_logits.view(-1,self.n_vocab) #bs*(ref_n_seq-1),n_vocab\n",
        "    \n",
        "    loss = F.cross_entropy(dec_logits, dec_target, ignore_index=self.pad_id)\n",
        "    \n",
        "    self.enc_optimizer.zero_grad()\n",
        "    self.dec_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.enc_optimizer.step()\n",
        "    self.dec_optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "  \n",
        "  def evaluate_rouge(self):\n",
        "    #ToDo Evaluate using Rouge\n",
        "    self.encoder.eval()\n",
        "    self.decoder.eval()\n",
        "    rouge = Rouge()\n",
        "    #------------------ \n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    for batch in self.val_dataloader:\n",
        "      batch_art_ids, batch_seq_len, batch_ref_ids, batch_spl_mask = batch\n",
        "      batch_art_ids = batch_art_ids.to(0) #bs, art_n_seq\n",
        "      batch_spl_mask = batch_spl_mask.to(0) #bs, art_n_seq\n",
        "      with T.autograd.no_grad():\n",
        "        enc_word_reps, enc_sent_rep = self.encoder(batch_art_ids, batch_seq_len)\n",
        "        dec_hidden = enc_sent_rep\n",
        "        if self.config.sampling_type == \"greedy\":\n",
        "          generated_tokens = self.generate_samples_greedy(enc_word_reps, batch_spl_mask, dec_hidden)\n",
        "        elif self.config.sampling_type == \"beamsearch\":\n",
        "          generated_tokens = self.generate_samples_beamsearch(enc_word_reps, batch_spl_mask, dec_hidden)\n",
        "      ref_dec = self.convert_ids_to_strings(generated_tokens, type_=\"ref\")\n",
        "      ref_gold = self.convert_ids_to_strings(batch_ref_ids, type_=\"ref\")\n",
        "      batch_rouge = rouge.get_scores(ref_gold, ref_dec, avg=True)\n",
        "      rouge1_scores.append(batch_rouge[\"rouge-1\"][\"f\"])\n",
        "      rouge2_scores.append(batch_rouge[\"rouge-2\"][\"f\"])\n",
        "      rougeL_scores.append(batch_rouge[\"rouge-l\"][\"f\"])\n",
        "    #------------------\n",
        "    self.encoder.train()\n",
        "    self.decoder.train()\n",
        "    return np.mean(rouge1_scores), np.mean(rouge2_scores), np.mean(rougeL_scores)\n",
        "  \n",
        "  def generate_samples_beamsearch(self, enc_word_reps, batch_spl_mask, dec_hidden):\n",
        "    batch_size, art_n_seq, n_hid = enc_word_reps.size()\n",
        "    beam_idx = T.LongTensor(list(range(batch_size)))\n",
        "    beams = [Beam(self.config, (dec_hidden[0][:,i,:], dec_hidden[1][:,i,:])) for i in range(batch_size)]\n",
        "    n_rem = batch_size\n",
        "    \n",
        "    for _ in range(self.config.ref_max_len):\n",
        "      prev_tokens = T.cat(\n",
        "          [beam.get_prev_tokens() for beam in beams if beam.done == False], dim=0\n",
        "      ) #rem*beam,1\n",
        "      dec_embeds = self.encoder.embeds(prev_tokens)\n",
        "      \n",
        "      dec_h = T.cat(\n",
        "          [beam.hid_h for beam in beams if beam.done == False], dim=1\n",
        "      ) #1,rem*beam,n_hid\n",
        "      dec_c = T.cat(\n",
        "          [beam.hid_c for beam in beams if beam.done == False], dim=1\n",
        "      ) #1,rem*beam,n_hid\n",
        "      dec_hidden = (dec_h, dec_c)\n",
        "      \n",
        "      enc_word_reps_beam = enc_word_reps[beam_idx].view(n_rem,-1).repeat(1,config.beam_size).view(-1, art_n_seq, n_hid)\n",
        "      batch_spl_mask_beam = batch_spl_mask[beam_idx].repeat(1,config.beam_size).view(-1, art_n_seq)\n",
        "      \n",
        "      dec_logits, (dec_h, dec_c) = self.decoder(dec_embeds, enc_word_reps_beam, batch_spl_mask_beam, dec_hidden)\n",
        "      log_probs = F.log_softmax(dec_logits, dim=2)\n",
        "      \n",
        "      log_probs = log_probs.view(n_rem, self.config.beam_size, self.config.n_vocab) #rem, beam, n_vocab\n",
        "      dec_h = dec_h.view(n_rem, config.beam_size, self.config.n_hid) #rem, beam, n_hid\n",
        "      dec_c = dec_c.view(n_rem, config.beam_size, self.config.n_hid) #rem, beam, n_hid\n",
        "      \n",
        "      active = []\n",
        "      for i in range(n_rem):\n",
        "        b = beam_idx[i].item()\n",
        "        beam = beams[b]\n",
        "        beam.advance(log_probs[i], (dec_h[i], dec_c[i]))\n",
        "        if beam.done == False:\n",
        "          active.append(b)\n",
        "\n",
        "      if len(active) == 0:\n",
        "        break\n",
        "\n",
        "      beam_idx = T.LongTensor(active)\n",
        "      n_rem = len(beam_idx)\n",
        "    \n",
        "    generated_tokens = []\n",
        "    for beam in beams:\n",
        "      generated_tokens.append(beam.get_best())\n",
        "    generated_tokens = nn.utils.rnn.pad_sequence(generated_tokens, batch_first=True, padding_value=self.vocab_obj.word2id(self.config.pad))\n",
        "    return generated_tokens\n",
        "      \n",
        "  def generate_samples_greedy(self, enc_word_reps, batch_spl_mask, dec_hidden):\n",
        "    #ToDo: Generate using Beam Search\n",
        "    bs = len(enc_word_reps)\n",
        "    cur_token = T.LongTensor(bs,1).fill_(self.bos_id).to(0)\n",
        "    generated_tokens = [cur_token]\n",
        "    for t in range(1,self.config.ref_max_len):\n",
        "      dec_embeds = self.encoder.embeds(cur_token)\n",
        "      dec_logits, dec_hidden = self.decoder(dec_embeds, enc_word_reps, batch_spl_mask, dec_hidden) #bs,1,n_vocab\n",
        "      word_dist = F.softmax(dec_logits.squeeze(1), dim=1) #bs,n_vocab\n",
        "      cur_token = T.multinomial(word_dist, 1)\n",
        "      generated_tokens.append(cur_token)\n",
        "    generated_tokens = T.cat(generated_tokens, dim=1) #bs,ref_max_len\n",
        "    return generated_tokens\n",
        "    \n",
        "  def convert_ids_to_strings(self, batch_ids, type_):\n",
        "      batch_ids = batch_ids.cpu().numpy()\n",
        "      strs = []\n",
        "      if type_ == \"article\":\n",
        "        end_id = self.pad_id\n",
        "      else:\n",
        "        end_id = self.eos_id\n",
        "      for i in range(len(batch_ids)):\n",
        "        sent_ids = batch_ids[i]\n",
        "        eos_tokens = np.where(sent_ids == end_id)\n",
        "        if len(eos_tokens[0]) > 0:\n",
        "          stop_token = eos_tokens[0][0]\n",
        "        else:\n",
        "          pad_tokens = np.where(sent_ids == self.pad_id)\n",
        "          if len(pad_tokens[0]) > 0:\n",
        "            stop_token = pad_tokens[0][0]\n",
        "          else:\n",
        "            stop_token = len(sent_ids)\n",
        "        sent_ids = sent_ids[1:stop_token] #Exclude cls, bos, sep, eos, pad tokens\n",
        "        if len(sent_ids) < 3:\n",
        "          sent_string = \"xxx\"\n",
        "        else:\n",
        "          sent_string = []\n",
        "          for id in sent_ids:\n",
        "            sent_string.append(self.vocab_obj.id2word(id))\n",
        "          sent_string = \" \".join(sent_string)\n",
        "        strs.append(sent_string)\n",
        "      return strs\n",
        "     \n",
        "  def save_model(self, itr, loss, final=False):\n",
        "    if final:\n",
        "      save_name = \"final.tar\"\n",
        "    else:\n",
        "      save_name = \"%06d.tar\"%(itr)\n",
        "    save_path = os.path.join(self.config.model_save_path, save_name)\n",
        "    T.save({\n",
        "        \"itr\": itr,\n",
        "        \"loss\": loss,\n",
        "        \"encoder\":self.encoder.state_dict(),\n",
        "        \"decoder\":self.decoder.state_dict(),\n",
        "        \"enc_optimizer\": self.enc_optimizer.state_dict(),\n",
        "        \"dec_optimizer\": self.dec_optimizer.state_dict()\n",
        "    }, save_path)\n",
        "    \n",
        "  def load_model(self, model_name, mode = \"train\"):\n",
        "    checkpoint = T.load(os.path.join(self.config.model_save_path, model_name))\n",
        "    self.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
        "    self.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
        "    if mode == \"train\":\n",
        "      self.enc_optimizer.load_state_dict(checkpoint[\"enc_optimizer\"])\n",
        "      self.dec_optimizer.load_state_dict(checkpoint[\"dec_optimizer\"])\n",
        "    return checkpoint[\"itr\"], checkpoint[\"loss\"]\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK6PAsJuSmrl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9b1e3a2-b60d-4c27-baf4-6b716dcf05c1"
      },
      "source": [
        "def training(model_name = None):\n",
        "  loss_arr = []\n",
        "  r1_arr = []\n",
        "  r2_arr = []\n",
        "  rL_arr = []\n",
        "  train_processor = Lstm_Lstm_Processor(config)\n",
        "  if model_name is None:\n",
        "    itr = 1\n",
        "  else:\n",
        "    itr, prev_loss = train_processor.load_model(model_name)\n",
        "    itr += 1\n",
        "    print(\"Loaded Model at:\",itr, \"; loss:\", prev_loss)\n",
        "  while(itr<=config.n_itr):\n",
        "    for batch in train_processor.train_dataloader:\n",
        "      try:\n",
        "        loss = train_processor.train_batch(batch)\n",
        "      except KeyboardInterrupt:\n",
        "        exit(0)\n",
        "      loss_arr.append(loss)\n",
        "      if itr%100 == 0:\n",
        "        print(\"itr:\", itr, \"loss:\", \"%.3f\"%(loss))\n",
        "      if itr%1500 == 0:\n",
        "        rouge1, rouge2, rougeL = train_processor.evaluate_rouge()\n",
        "        r1_arr.append(rouge1)\n",
        "        r2_arr.append(rouge2)\n",
        "        rL_arr.append(rougeL)\n",
        "        print(\"val scores:-\", \"Rouge-1:\",\"%.3f\"%(rouge1), \"Rouge-2:\",\"%.3f\"%(rouge2), \"Rouge-L:\",\"%.3f\"%(rougeL))\n",
        "      if itr%5000 == 0:\n",
        "        train_processor.save_model(itr, loss, final=True)\n",
        "      if itr == config.n_itr:\n",
        "        print(\"Reached\")\n",
        "        T.save({\n",
        "              \"loss_arr\" : loss_arr,\n",
        "              \"r1_arr\" : r1_arr,\n",
        "              \"r2_arr\" : r2_arr,\n",
        "              \"rL_arr\" : rL_arr,\n",
        "          }, \"/content/gdrive/My Drive/585/scores.tar\")\n",
        "        print(\"Saved\")\n",
        "      itr += 1\n",
        "      if itr > config.n_itr:\n",
        "        break\n",
        "        \n",
        "def testing(model_name):\n",
        "  rouge = Rouge()\n",
        "  train_processor = Lstm_Lstm_Processor(config, mode=\"test\")\n",
        "  itr, prev_loss = train_processor.load_model(model_name, mode = \"test\")\n",
        "  print(\"Loaded Model at:\",itr, \"; loss:\", prev_loss)\n",
        "  for batch in train_processor.test_dataloader:\n",
        "    batch_art_ids, batch_seq_len, batch_ref_ids, batch_spl_mask = batch\n",
        "    batch_art_ids = batch_art_ids.to(0) #bs, art_n_seq\n",
        "    batch_spl_mask = batch_spl_mask.to(0) #bs, art_n_seq\n",
        "    \n",
        "    with T.autograd.no_grad():\n",
        "      enc_word_reps, enc_sent_rep = train_processor.encoder(batch_art_ids, batch_seq_len)\n",
        "      dec_hidden = enc_sent_rep\n",
        "      if train_processor.config.sampling_type == \"greedy\":\n",
        "        generated_tokens = train_processor.generate_samples_greedy(enc_word_reps, batch_spl_mask, dec_hidden)\n",
        "      elif train_processor.config.sampling_type == \"beamsearch\":\n",
        "        generated_tokens = train_processor.generate_samples_beamsearch(enc_word_reps, batch_spl_mask, dec_hidden)\n",
        "    ref_dec_strs = train_processor.convert_ids_to_strings(generated_tokens, type_=\"ref\")\n",
        "    article_strs = train_processor.convert_ids_to_strings(batch_art_ids, type_=\"article\")\n",
        "    ref_gold_strs = train_processor.convert_ids_to_strings(batch_ref_ids, type_=\"ref\")\n",
        "    for article, ref_dec, ref_gold in zip(article_strs, ref_dec_strs, ref_gold_strs):\n",
        "      print(\"Article:\", article)\n",
        "      print(\"Ref_dec:\", ref_dec)\n",
        "      print(\"Ref_gold:\", ref_gold)\n",
        "      scores = rouge.get_scores(ref_gold, ref_dec, avg=True)\n",
        "      print(\"Rouge:\", scores[\"rouge-l\"][\"f\"])\n",
        "      print(\"--------------------------\")\n",
        "    break\n",
        "    \n",
        "#training()\n",
        "testing('final.tar')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Model at: 30000 ; loss: 3.3685338497161865\n",
            "Article: activists from throughout the west plan to converge on northern california town of <unk> sunday for the third annual rally to save <unk> forest .\n",
            "Ref_dec: upper house maathai _ in straight west event failing singers west\n",
            "Ref_gold: environmental groups plan protest at forest of <unk>\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: an arab world allergic to political reform , the new iraq taking shape under u.s. <unk> is a troubling harbinger .\n",
            "Ref_dec: arab world bank promotes assessment of iraqi corruption\n",
            "Ref_gold: to <unk> arab rulers new iraq is an unwanted role model\n",
            "Rouge: 0.1003798155174875\n",
            "--------------------------\n",
            "Article: 's a car that answers questions that nobody was asking , a vehicle that looks as if it were the work of the of <unk> or disney studios rather than an exercise in automotive design .\n",
            "Ref_dec: james astronomers proves what they design the escalation\n",
            "Ref_gold: suzuki 's <unk> mini poses a major question _ what were they\n",
            "Rouge: 0.18571428571359508\n",
            "--------------------------\n",
            "Article: schumacher and ferrari capped a record-breaking year sunday when the german powered the italian car to win the japanese grand prix for his ##th victory of the formula one season .\n",
            "Ref_dec: schumacher back on record year at the block\n",
            "Ref_gold: schumacher leads ferrari #-# for ##th win of the season\n",
            "Rouge: 0.21693121693055348\n",
            "--------------------------\n",
            "Article: shares closed down #.## percent monday , hit by negative sentiment from wall street and local profit taking , brokers said .\n",
            "Ref_dec: australian shares close down #.# percent\n",
            "Ref_gold: australian shares close down #.## percent on us lead\n",
            "Rouge: 0.6710526315782787\n",
            "--------------------------\n",
            "Article: 's army chief of staff won political leaders ' backing monday for his overthrow of the increasingly unpopular and erratic president , who remained in military detention after an apparently bloodless coup .\n",
            "Ref_dec: guinea-bissau 's coup leaders <unk> scores\n",
            "Ref_gold: guinea-bissau coup leaders promise civilian transition government\n",
            "Rouge: 0.4561717352408751\n",
            "--------------------------\n",
            "Article: light earthquake shook mexico sunday morning , but no major damage or injuries were reported .\n",
            "Ref_dec: mild tremor shakes mexico 's <unk>\n",
            "Ref_gold: light earthquake shakes mexico\n",
            "Rouge: 0.371428571428493\n",
            "--------------------------\n",
            "Article: 's six main political parties agreed to join a national unity cabinet in the hope of ending a ##-year political impasse that has prevented parliamentary elections in the west african nation .\n",
            "Ref_dec: togo political parties agree to national unity party\n",
            "Ref_gold: togo to create multiparty commission to move toward long-delayed parliamentary vote\n",
            "Rouge: 0.21693121693055348\n",
            "--------------------------\n",
            "Article: tropical paradise , a discotheque , a wedding hall , a corner cafe : more and more , terrorists are hitting home by hitting civilians away from home .\n",
            "Ref_dec: san jose exits of first house object to job at home\n",
            "Ref_gold: bali bombings underscore terrorists attraction to soft targets with <unk>\n",
            "Rouge: 0.09480909480869641\n",
            "--------------------------\n",
            "Article: ### travelers have been stranded in turkey after a charter airline company canceled flights to paris over a dispute with a travel agency , french officials said monday .\n",
            "Ref_dec: striking <unk> officials turn in government flights in wake of paris nightclub flights\n",
            "Ref_gold: french tourists stranded in turkey over dispute between agency airline company\n",
            "Rouge: 0.09090909090859091\n",
            "--------------------------\n",
            "Article: evangelical christians , the directive is clearly laid forth in the <unk> .\n",
            "Ref_dec: christians gather for fertility margin\n",
            "Ref_gold: evangelical leader to speak at conference\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: truck filled with explosives that police believe may have been destined for kabul blew up on a highway thursday , killing ## people -- more than half of them children walking to school .\n",
            "Ref_dec: snowstorm leaves more olympics for more than a year ago ; # killed\n",
            "Ref_gold: afghan blast kills ## half of them children\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: minister john howard on friday defended a string of raids by security forces on indonesian muslims living in australia after officials in jakarta complained .\n",
            "Ref_dec: australian prime minister defends indonesian leaders on australian raids\n",
            "Ref_gold: australian prime minister defends raids on indonesian muslims after jakarta complains\n",
            "Rouge: 0.501899077590195\n",
            "--------------------------\n",
            "Article: environmental advocacy group says tourism and fishing deliver $ ### billion annually to coastal economies along the pacific , atlantic and florida gulf shores -- about four times the value of the estimated oil and gas that could be recovered every year from proposed offshore drilling nearby .\n",
            "Ref_dec: order that grow and hurricane damage to be snags along nebraska debts\n",
            "Ref_gold: group says coastal tourism fishing beat oil\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: police said wednesday they have arrested a libyan man suspected in the #### bombing of a berlin disco that killed two u.s. soldiers and a turkish woman .\n",
            "Ref_dec: police arrest ## in disco bomb probe\n",
            "Ref_gold: italian police arrest libyan in '## berlin disco bombing\n",
            "Rouge: 0.4850746268649977\n",
            "--------------------------\n",
            "Article: 's army has suspended seven officers and soldiers for failing to control their troops in connection with the brutal murders last month of three impoverished children near colombia 's northeast border with venezuela .\n",
            "Ref_dec: seven days suspended seven month suspended for boarding killings of colombian army officers\n",
            "Ref_gold: colombia suspends # in military after children s killings\n",
            "Rouge: 0.09805825242690437\n",
            "--------------------------\n",
            "Article: measures that have been taken at athens international airport in view of the olympic games slated for august ## - ## , were unveiled tuesday by the airport administration .\n",
            "Ref_dec: athens airport to fight from cds\n",
            "Ref_gold: measures for athens international airport during olympic games unveiled\n",
            "Rouge: 0.247619047618357\n",
            "--------------------------\n",
            "Article: man who had contested the legality of lethal injections was executed friday in indiana after the us supreme court refused to reconsider his case .\n",
            "Ref_dec: ohio executes indiana execution pending\n",
            "Ref_gold: us executes man who contested lethal injection\n",
            "Rouge: 0.15811965811896683\n",
            "--------------------------\n",
            "Article: 's computer market brought in ### million u.s. dollars in #### , the vietnam news reported friday .\n",
            "Ref_dec: vietnam reports first-ever computer market\n",
            "Ref_gold: vietnam computer market earns ### mln dollars in ####\n",
            "Rouge: 0.3723653395778134\n",
            "--------------------------\n",
            "Article: lost singles and doubles matches to the dominican republic sunday , but both countries advanced to group # in qualifying play among countries in the davis cup 's american division 's group # .\n",
            "Ref_dec: americans want aim to play group singles\n",
            "Ref_gold: dominican republic defeats haiti in davis cup qualifying play but\n",
            "Rouge: 0.11094564407972461\n",
            "--------------------------\n",
            "Article: investigators launched an investigation monday to determine whether <unk> , possibly tainted with the hepatitis a virus , were illegally sold to the u.s. department of agriculture from mexico .\n",
            "Ref_dec: feds gathered over sleaze calls for u.s. review in <unk>\n",
            "Ref_gold: investigators probe possible mexican origin of <unk>\n",
            "Rouge: 0.10155316606925241\n",
            "--------------------------\n",
            "Article: star jan <unk> is possibly one the most popular signings ever among liverpool fans , without even playing a game yet .\n",
            "Ref_dec: <unk> calm and liverpool manages a rare success\n",
            "Ref_gold: new boy <unk> already a hero at anfield\n",
            "Rouge: 0.2499999999995\n",
            "--------------------------\n",
            "Article: kansas city royals placed starting pitcher chris <unk> on the ##-day disabled list with a broken left ankle .\n",
            "Ref_dec: royals place royals for royals\n",
            "Ref_gold: royals place chris <unk> on disabled list with broken ankle\n",
            "Rouge: 0.2122687439139677\n",
            "--------------------------\n",
            "Article: commission president romano prodi promised russian prime minister mikhail kasyanov on wednesday the european union will try and settle the dispute over the russian enclave of kaliningrad ahead of its eastward expansion .\n",
            "Ref_dec: eu head meets pensioners of enclave and eu force\n",
            "Ref_gold: eu promises to try and solve kaliningrad issue ahead of eu\n",
            "Rouge: 0.21693121693055348\n",
            "--------------------------\n",
            "Article: seamer <unk> <unk> claimed the vital wickets of brian lara and carl hooper for india but the west indies bounced back to leave the second cricket test hanging in the balance at lunch on the fifth and final day tuesday .\n",
            "Ref_dec: n west indies crush west indies\n",
            "Ref_gold: <unk> strikes two big blows for india before west indies fight\n",
            "Rouge: 0.19641577060888848\n",
            "--------------------------\n",
            "Article: china 's shanxi province , the country 's largest coal producer , shut down #,### illegal coal mines last year .\n",
            "Ref_dec: shanxi province found north china province\n",
            "Ref_gold: china 's <unk> province shuts down #,### illegal mines\n",
            "Rouge: 0.2482435597183287\n",
            "--------------------------\n",
            "Article: kong is to stage the passionate and romantic opera `` carmen '' at the hong kong cultural center grand theater from ## september to # october .\n",
            "Ref_dec: hk to stage its centenary at mardi gras\n",
            "Ref_gold: opera carmen to be staged in hong kong\n",
            "Rouge: 0.1249999999995\n",
            "--------------------------\n",
            "Article: zealand share prices closed #.## percent higher monday in trade disrupted by technical problems , dealers said .\n",
            "Ref_dec: new zealand shares close #.## percent higher\n",
            "Ref_gold: new zealand shares close #.## percent higher\n",
            "Rouge: 0.9999999999995\n",
            "--------------------------\n",
            "Article: contador all but sealed his second tour de france victory by keeping the yellow jersey after saturday 's punishing penultimate stage , and lance armstrong remained in third overall .\n",
            "Ref_dec: contador retains yellow jersey yet again\n",
            "Ref_gold: contador all but locks up tour victory\n",
            "Rouge: 0.1520572450798733\n",
            "--------------------------\n",
            "Article: imperial bank of commerce 's fiscal third-quarter profit rose a <unk> ## percent on increased <unk> and underwriting fees , mortgages , commercial loans and trading commissions .\n",
            "Ref_dec: canadian trade and and profit operating profit expectations on <unk> reports\n",
            "Ref_gold: canadian imperial bank of commerce fiscal #rd-qtr net up ## %\n",
            "Rouge: 0.09805825242653109\n",
            "--------------------------\n",
            "Article: us airways is unable to reach a cost-cutting agreement with its labor unions , it may start dropping unprofitable routes between the east and west coasts , as well as routes in florida and at its baltimore hub .\n",
            "Ref_dec: pixar may end strike\n",
            "Ref_gold: us airways considering downsizing\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: mainland chinese jet with ### people on board returned home from here late monday after taiwan police detained the lone air pirate , witnesses said .\n",
            "Ref_dec: chinese mainland china board recovers on <unk> plane\n",
            "Ref_gold: hijacked chinese plane returns home hijacker held\n",
            "Rouge: 0.2643274853797614\n",
            "--------------------------\n",
            "Article: <unk> cooke , riding for the <unk> team , won the grand prix la <unk> on tuesday , raced over ###km between <unk> and here .\n",
            "Ref_dec: <unk> out of winning fashion race\n",
            "Ref_gold: australian cooke wins grand prix la <unk>\n",
            "Rouge: 0.1520572450798733\n",
            "--------------------------\n",
            "Article: and turkish officials discussed possible actions against kurdish rebels in northern iraq , a key turkish request to send troops to iraq , news reports said friday .\n",
            "Ref_dec: turks kurdish rebels to send warships kurds to turkey into iraq during more steps\n",
            "Ref_gold: turkey u.s. discuss possible measures against kurdish rebels in northern iraq\n",
            "Rouge: 0.3189808917193299\n",
            "--------------------------\n",
            "Article: a year after seizing power in a military <unk> that ousted mauritania 's first freely elected leader , mohamed ould abdel aziz won the presidency sunday in a landslide vote his opponents decried as a fraudulent `` electoral coup .\n",
            "Ref_dec: landslide take largest in africa coup\n",
            "Ref_gold: mauritania coup chief wins vote amid fraud claims\n",
            "Rouge: 0.13736263736195337\n",
            "--------------------------\n",
            "Article: policemen have been killed and four others injured in a conflict in uruzgan province of southern afghanistan , a spokesman for provincial governor told xinhua on monday .\n",
            "Ref_dec: twelve afghan policemen died in southern afghanistan\n",
            "Ref_gold: # policemen killed in clash with militants in s. afghanistan\n",
            "Rouge: 0.36380597014857974\n",
            "--------------------------\n",
            "Article: people were killed and ## others injured when a strong earthquake shook southern iran on monday , the news media reported .\n",
            "Ref_dec: # killed two injured in iran earthquake\n",
            "Ref_gold: two killed ## injured as strong quake hits southern iran\n",
            "Rouge: 0.3328369322405586\n",
            "--------------------------\n",
            "Article: a tribute to his grandmother , prince charles praised the late queen mother elizabeth for her lifelong popularity , which was obvious across britain on monday as people left teddy bears , candles and wreaths outside royal sites such windsor castle in her honor .\n",
            "Ref_dec: mother walk queen mother for queen elizabeth queen elizabeth ii mother\n",
            "Ref_gold: across britain public mourns the passing of the popular queen mother\n",
            "Rouge: 0.2236842105256471\n",
            "--------------------------\n",
            "Article: south african government may reopen the inquest into the death of charismatic black leader steve <unk> , justice minister <unk> omar told the sunday independent , after several <unk> confessed to the killing .\n",
            "Ref_dec: south africa recognizes new black camp\n",
            "Ref_gold: south african government says new inquiry into <unk> death possible\n",
            "Rouge: 0.2236842105256471\n",
            "--------------------------\n",
            "Article: george w. bush won a forceful victory over sen. john mccain in virginia and north dakota on tuesday , picking up a large cache of delegates as well as a considerable morale boost after his discouraging loss in michigan a week ago .\n",
            "Ref_dec: virginia governor missed chances to close virginia 's film\n",
            "Ref_gold: bush wins in virginia and north dakota ; takes lead in washington\n",
            "Rouge: 0.1003798155174875\n",
            "--------------------------\n",
            "Article: asian stock markets closed lower monday , but bucking the trend was the tokyo stock exchange , where the key index edged higher .\n",
            "Ref_dec: asian stock markets close lower\n",
            "Ref_gold: most asian stock markets close lower\n",
            "Rouge: 0.8944281524920247\n",
            "--------------------------\n",
            "Article: total of ## british athletes and training staff arrived in <unk> in southwest cyprus late friday where the british team will make its final preparations before the athens olympics next month .\n",
            "Ref_dec: briton arrives in greece ahead of olympic mission\n",
            "Ref_gold: british athletes arrive in cyprus for training\n",
            "Rouge: 0.13216374268970296\n",
            "--------------------------\n",
            "Article: dollar kept upward momentum thursday as the euro was pressured by growing speculation on an interest rate cut by the european central bank .\n",
            "Ref_dec: ecb holds key rate lower\n",
            "Ref_gold: dollar gains on more speculation of cut in eurozone rates\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: 's accelerating economic growth and its rising foreign investment justify the <unk> ` baa ' rating on its foreign currency debt , moody 's investors service said .\n",
            "Ref_dec: croatia 's foreign currency changed\n",
            "Ref_gold: croatia merits <unk> debt rating moody 's says\n",
            "Rouge: 0.2794348508627425\n",
            "--------------------------\n",
            "Article: <unk> institute decided today to award the nobel prize in <unk> or medicine jointly to edward b. lewis , <unk> <unk> and eric f. <unk> .\n",
            "Ref_dec: nobel prize winner <unk> honored\n",
            "Ref_gold: #### nobel prize for medicine awarded to three scientists\n",
            "Rouge: 0.2482435597183287\n",
            "--------------------------\n",
            "Article: russian <unk> ' union will launch a national strike on march # if the government does not take urgent steps to pay the miners the arrears it owes them , union president vitali <unk> said friday .\n",
            "Ref_dec: union feud on mortgage crop\n",
            "Ref_gold: russian <unk> threaten national strike\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: years of partisan warfare over balancing the federal budget , the white house and congressional republicans enter this week closer to a deal than ever before .\n",
            "Ref_dec: white house cox sails to racers of the press democrat\n",
            "Ref_gold: budget negotiators step gingerly toward deal\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: prosecution in the dog mauling trial of a san francisco couple on tuesday attacked a defendant 's version of the events , suggesting she lied when she testified that she threw herself on top of her neighbor to shield her from a rampaging ###-pound <unk> <unk> .\n",
            "Ref_dec: s.f. awards couple slam access to she was collide on her anthem tells her they try\n",
            "Ref_gold: state attacks defendant 's testimony on dog mauling\n",
            "Rouge: 0.06944444444451853\n",
            "--------------------------\n",
            "Article: people were killed on friday when the cable of a construction elevator snapped and fell ## meters in hangzhou , capital of east china 's zhejiang province .\n",
            "Ref_dec: # die in cable car construction\n",
            "Ref_gold: four killed after lift falls in east china\n",
            "Rouge: 0.13736263736195337\n",
            "--------------------------\n",
            "Article: special court set up by khartoum in a bid to <unk> an international probe into alleged war crimes in darfur will hold its first hearings on saturday , the national news agency announced thursday .\n",
            "Ref_dec: new darfur hearings partially unlikely first in court\n",
            "Ref_gold: sudan 's darfur crimes court to hold first hearing saturday\n",
            "Rouge: 0.21693121693055348\n",
            "--------------------------\n",
            "Article: agricultural hi-tech zone in shaanxi province , which was set up as a demonstration zone for dry farming in northwest china , is making progress .\n",
            "Ref_dec: shannon zone diversion starts indefinite murder zone in northwest\n",
            "Ref_gold: shaanxi steps up construction of agricultural hi-tech zone\n",
            "Rouge: 0.1249999999995\n",
            "--------------------------\n",
            "Article: number of palestinian factions considered on thursday that the retreat in the number of <unk> attacks against israeli targets was due to security reasons .\n",
            "Ref_dec: <unk> attacks get from hospitals for attacks\n",
            "Ref_gold: palestinian factions say retreat of their attacks due to security reasons\n",
            "Rouge: 0.10148674854493898\n",
            "--------------------------\n",
            "Article: confirmed monday that the basis of an agreement has been reached with libya on compensation for families of victims in a #### passenger jet bombing .\n",
            "Ref_dec: france confirms passenger deal sunday\n",
            "Ref_gold: france confirms agreement with libya on airline bombing\n",
            "Rouge: 0.2794348508627425\n",
            "--------------------------\n",
            "Article: 's health minister timothy stamps says about ###,### people in zimbabwe -- one percent of the country 's population -- will die of <unk> diseases within the next ## months .\n",
            "Ref_dec: zimbabwe s monthly annan vows to feed ills diseases at least ###,### people\n",
            "Ref_gold: ###,### zimbabweans face <unk> deaths\n",
            "Rouge: 0.0835486649441183\n",
            "--------------------------\n",
            "Article: `` <unk> , '' the <unk> snake movie that opens today , director luis llosa pilots a film that feels like a jungle cruise ride into the pit of hell .\n",
            "Ref_dec: <unk> les offers something into olympics\n",
            "Ref_gold: <unk> <unk> with laughs\n",
            "Rouge: 0.18518518518525923\n",
            "--------------------------\n",
            "Article: british woman finished a ###-day trip across the outback on saturday , in what was believed to be the first australian crossing by a female on horseback .\n",
            "Ref_dec: pregnant woman granted australian first time in on first female british woman\n",
            "Ref_gold: briton believed to be first woman to cross australia on horseback\n",
            "Rouge: 0.19999999999950002\n",
            "--------------------------\n",
            "Article: <unk> , a renowned <unk> poet , died in hospital friday after a prolonged illness , his doctor said .\n",
            "Ref_dec: renowned <unk> poet <unk> dies\n",
            "Ref_gold: renowned indian poet dies at age ##\n",
            "Rouge: 0.4791154791148272\n",
            "--------------------------\n",
            "Article: review and commentary on consumer reports magazine 's auto testing generated a lot of comment , mostly positive , some negative .\n",
            "Ref_dec: reading on the rise to achieve murder\n",
            "Ref_gold: taking a little heat on consumer reports testing\n",
            "Rouge: 0.1321637426894444\n",
            "--------------------------\n",
            "Article: dollar was easier against major currencies in mid-afternoon tokyo trading wednesday , with investors reluctant to take fresh positions ahead of us trade data to be released later in the day , dealers said .\n",
            "Ref_dec: dollar firms against clinton mid-afternoon tokyo\n",
            "Ref_gold: dollar easier in mid-afternoon tokyo trade\n",
            "Rouge: 0.4999999999995\n",
            "--------------------------\n",
            "Article: a bombing that injured three russian peacekeepers in georgia 's breakaway region of abkhazia , russia accused the georgian government on tuesday of fueling tension in the black sea province .\n",
            "Ref_dec: russian separatists destroy official calls for bombing georgian s dagestan\n",
            "Ref_gold: russia lashes out at georgia after attack on peacekeepers in abkhazia\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: stocks fell as federal reserve gov. laurence meyer said the economy is growing at an `` unsustainable '' pace , signaling that more u.s. interest rates increases are likely .\n",
            "Ref_dec: u.s. stocks fall on fed plans for fed s <unk> chairman\n",
            "Ref_gold: u.s. stocks fall as fed governor signals higher interest rates\n",
            "Rouge: 0.4740454740448881\n",
            "--------------------------\n",
            "Article: was plunged into virtual isolation thursday as its telephone system suffered a major breakdown , cutting off all international circuits for nearly the entire business day .\n",
            "Ref_dec: cambodian government launches restructuring test as plunges\n",
            "Ref_gold: cambodia plunged into isolation as all international phone lines cut\n",
            "Rouge: 0.11094564407972461\n",
            "--------------------------\n",
            "Article: pitcher <unk> <unk> went six complete innings sunday to pick up his first professional win as the pacific league-leading daiei hawks defeated the orix bluewave #-# .\n",
            "Ref_dec: league-leading blanks orix to winning start nippon series\n",
            "Ref_gold: daiei rookie <unk> collects first win\n",
            "Rouge: 0.0\n",
            "--------------------------\n",
            "Article: wells stood on the mound and <unk> his glove , frustrated at the job he had done in less than six innings against the boston red sox at yankee stadium on friday .\n",
            "Ref_dec: yankees top ## wells to mark the lead\n",
            "Ref_gold: to fans glove from disgusted wells is no <unk>\n",
            "Rouge: 0.11684125705016256\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0N88FrUsPFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a345a3fd-0c71-44e1-adae-b2b00c891be3"
      },
      "source": [
        "\n",
        "checkpoint = T.load(\"/content/gdrive/My Drive/585/scores.tar\")\n",
        "r1_arr = checkpoint['r1_arr']\n",
        "r2_arr = checkpoint['r2_arr']\n",
        "rL_arr = checkpoint['rL_arr']\n",
        "print(r1_arr)\n",
        "print(r2_arr)\n",
        "print(rL_arr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.13621861659302445, 0.16473979876362838, 0.18061692359923553, 0.1882501750881013, 0.20061959359603773, 0.20510242863629807, 0.20455946265891164, 0.21113906247058623, 0.21272430555160265, 0.21368561235318276, 0.21485634182413577, 0.21691521279985204, 0.2170823799633546, 0.21997096424093637, 0.22092137285084626, 0.22883802465554742, 0.2280097737350058, 0.2267971646173757, 0.22373036149928532, 0.22956357959349213]\n",
            "[0.029169209267163185, 0.03952115948404685, 0.04502421233379681, 0.04919413955886631, 0.05387434826295175, 0.055499176667447615, 0.05541699188861595, 0.05705860236964505, 0.05742737568478614, 0.05967013287409832, 0.05917406764442135, 0.058795905986917946, 0.06017710124766253, 0.061028479943143095, 0.061538128533290784, 0.06555909150105699, 0.0653074998071326, 0.06551922877875167, 0.06284572452046142, 0.06679438939162441]\n",
            "[0.12254772996923641, 0.14737655693368631, 0.16186077479085428, 0.16786310096422552, 0.17913072381495598, 0.18305395982991507, 0.18262049144133208, 0.18847666127531199, 0.18902125257788002, 0.18952547242885112, 0.19055680751933102, 0.19254696797677834, 0.19361512182562277, 0.19556291487628089, 0.1959950861226908, 0.20352300564280462, 0.20216199374098454, 0.20133863802011878, 0.19824051973826104, 0.20335748903271023]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}